{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related libraries\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import metrics\n",
    "\n",
    "# set random seeds to ensure reproducibility\n",
    "import random\n",
    "random.seed(5500)\n",
    "np.random.seed(5500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some useful utility functions\n",
    "def get_paths():\n",
    "    paths = json.loads(open(\"SETTINGS.json\").read())\n",
    "    return paths\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "# trigger the columns to be parsed as strings or specified data types\n",
    "converters = { \"FullDescription\" : identity, \"Title\": identity, \"LocationRaw\": identity, \"LocationNormalized\": identity}\n",
    "\n",
    "def get_train_df():\n",
    "    train_path = get_paths()[\"train_data_path\"]\n",
    "    return pd.read_csv(train_path) # converters=converters\n",
    "\n",
    "def get_valid_df():\n",
    "    valid_path = get_paths()[\"valid_data_path\"]\n",
    "    return pd.read_csv(valid_path) # converters=converters\n",
    "\n",
    "def get_test_df():\n",
    "    test_path = get_paths()[\"test_data_path\"]\n",
    "    return pd.read_csv(test_path) # converters=converters\n",
    "\n",
    "def save_model(model, filename):\n",
    "    model_path = get_paths()[\"model_path\"]\n",
    "    dump(model, model_path + filename)\n",
    "\n",
    "def load_model(filename):\n",
    "    model_path = get_paths()[\"model_path\"]\n",
    "    return load(model_path + filename)\n",
    "\n",
    "def write_submission(ids, predictions, filename):\n",
    "    prediction_path = get_paths()[\"prediction_path\"]\n",
    "    output_dict = {\"Id\": ids, \"SalaryNormalized\": predictions}\n",
    "    output = pd.DataFrame(output_dict)\n",
    "    output.to_csv(prediction_path + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class FeatureMapper for extracting features for different columns automatically in the pipeline\n",
    "class FeatureMapper:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            extractor.fit(X[column_name], y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            fea = extractor.transform(X[column_name])\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray()) # convert sparse matrix into dense numpy ndarray\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1) # concatenate columns\n",
    "        else: \n",
    "            return extracted[0]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            fea = extractor.fit_transform(X[column_name], y)\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uni-gram CountVectorizer\n",
    "def get_unigram_CountVectorizer(max_features):\n",
    "    return CountVectorizer(max_features=max_features)\n",
    "\n",
    "# get uni-gram TfidfVectorizer\n",
    "def get_unigram_TfidfVectorizer(max_features):\n",
    "    return TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# get binary uni-gram CountVectorizer for models like Bernoulli Naive Bayes\n",
    "def get_binary_CountVectorizer(max_features):\n",
    "    return CountVectorizer(binary=True, max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions for building a data pipeline\n",
    "def default_feature_extractor():\n",
    "    features = [('FullDescription-Bag of Words', 'FullDescription', CountVectorizer(max_features=100)),\n",
    "                ('Title-Bag of Words', 'Title', CountVectorizer(max_features=100)),\n",
    "                ('LocationRaw-Bag of Words', 'LocationRaw', CountVectorizer(max_features=100)),\n",
    "                ('LocationNormalized-Bag of Words', 'LocationNormalized', CountVectorizer(max_features=100))]\n",
    "    # max_features: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    combined = FeatureMapper(features)\n",
    "    return combined\n",
    "\n",
    "def default_classifier():\n",
    "    return RandomForestRegressor(n_estimators=50, verbose=2, n_jobs=1, min_samples_split=30, random_state=3465343)\n",
    "\n",
    "# get a pipeline to automatically extract features and train models\n",
    "def get_pipeline(features, clf):\n",
    "    steps = [(\"extract_features\", features), (\"classify\", clf)] # use memory parameter of pipeline to cache transformer since fitting \n",
    "                                                                # transformers could be expensive.\n",
    "    return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "# remove Id, LocationRaw, Company, SalaryRaw, SourceName variables before model training\n",
    "train = train_data.drop(columns=[\"Id\", \"LocationRaw\", \"Company\", \"SalaryRaw\", \"SourceName\"])\n",
    "\n",
    "# treat SalaryNormalized as target variable\n",
    "X = train.drop(columns=\"SalaryNormalized\")\n",
    "y = train_data.SalaryNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train h2o deep network model\n",
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train xgboost model\n",
    "import xgboost as xgb\n",
    "data = X\n",
    "label = pandas.DataFrame(y)\n",
    "dtrain = xgb.DMatrix(data, label=label)\n",
    "\n",
    "# missing values can be replaced by a default value in the DMatrix constructor\n",
    "# dtrain = xgb.DMatrix(data, label=label, missing=-999.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save model\n",
    "print(\"Reading in the training data\")\n",
    "train = get_train_df()\n",
    "\n",
    "print(\"Extracting features and training model\")\n",
    "benchmark_pipeline = get_pipeline(default_feature_extractor(), default_classifier())\n",
    "benchmark_pipeline.fit(train, train[\"SalaryNormalized\"])\n",
    "\n",
    "# print(\"Saving the fitted pipeline\")\n",
    "# save_model(benchmark_pipeline, \"benchmark_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for a model\n",
    "print(\"Loading the classifier\")\n",
    "# classifier = load_model(\"benchmark_pipeline.joblib\")\n",
    "classifier = benchmark_pipeline\n",
    "\n",
    "print(\"Making predictions on train set\")\n",
    "train = get_train_df()\n",
    "y_pred_test_benchmark = classifier.predict(train)\n",
    "\n",
    "print(\"Making predictions on validation set\") \n",
    "valid = get_valid_df()\n",
    "y_pred_valid_benchmark = classifier.predict(valid) # a single line to apply transform and predict, no fit.\n",
    "                                        # the same transforming (as to the train set) to the validation set and predict salary.\n",
    "# y_pred_valid_benchmark = y_pred_valid_benchmark.reshape(len(y_pred_valid_benchmark), 1)\n",
    "\n",
    "print(\"Making predictions on test set\")\n",
    "test = get_test_df()\n",
    "y_pred_test_benchmark = classifier.predict(test)\n",
    "\n",
    "# write predictions into local csv file for submission\n",
    "# print(\"Writing predictions to file\")\n",
    "# write_submission(ids, predictions, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print performance metrics for each model on train set\n",
    "train = get_train_df()\n",
    "def get_mae_train(clf_name, predictions):\n",
    "    print(\"The MAE for %s on train set is %.4f\" % (clf_name, metrics.mean_absolute_error(train.SalaryNormalized, predictions)))\n",
    "    \n",
    "get_mae_train(\"xgboost\", y_pred_train_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print performance metrics for each model on validation set\n",
    "valid = get_valid_df()\n",
    "def get_mae_valid(clf_name, predictions):\n",
    "    print(\"The MAE for %s on validation set is %.4f\" % (clf_name, metrics.mean_absolute_error(valid.SalaryNormalized, predictions)))\n",
    "    \n",
    "get_mae_valid(\"xgboost\", y_pred_valid_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print performance metrics for each model on test set\n",
    "test = get_test_df()\n",
    "def get_mae_test(clf_name, predictions):\n",
    "    print(\"The MAE for %s on test set is %.4f\" % (clf_name, metrics.mean_absolute_error(test.SalaryNormalized, predictions)))\n",
    "    \n",
    "get_mae_test(\"xgboost\", y_pred_test_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
